import json
import os
import re
import time
import requests
from datetime import datetime
from threading import Lock
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
CHROME_DRIVER_PATH = ChromeDriverManager().install()

# ---------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ----------
INPUT_FILE = "stage10_models_detailed.json"
FAILED_FILE = "stage11_failed.json"
OUTPUT_FILE = "stage11_modifications_detailed.json"
PROXY_FILE = "proxies_cleaned.txt"
TMP_DIR = "stage11_temp_results"
LOG_DIR = "zapo_logs"
THREADS_REQUESTS = 100
THREADS_SELENIUM = 10
PAGE_TIMEOUT = 30
RETRIES_REQUESTS = 10 
MIRRORS = [
    "https://part.avtomir.ru",
    "https://zapo.ru",
    "https://vindoc.ru",
    "https://autona88.ru",
    "https://b2b.autorus.ru",
    "https://xxauto.pro",
    "https://motexc.ru"
    ]

def with_mirror(url, mirror):
    return re.sub(r"https://[^/]+", mirror, url)

def is_rate_limited(html_text):
    return '–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å' in html_text

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(TMP_DIR, exist_ok=True)
log_file_path = os.path.join(LOG_DIR, f"stage11_log_{datetime.now():%Y%m%d_%H%M%S}.txt")
log_lock = Lock()
save_lock = Lock()

good_proxies = []
used_proxies = set()
requests_phase_results = []
failed_items = []

def extract_expected_modifications(soup):
    divs = soup.find_all("div")
    for div in divs:
        if "–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π:" in div.text:
            match = re.search(r"–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π:\s*(\d+)", div.text)
            if match:
                return int(match.group(1))
    return None

def get_pages_total_actual(soup):
    pages = soup.select("a.pageNumber.selectFilterPage")
    return max([int(a.text.strip()) for a in pages if a.text.strip().isdigit()], default=1)

def log(msg):
    print(msg)
    with log_lock:
        with open(log_file_path, "a", encoding="utf-8") as f:
            f.write(msg + "\n")

def safe_filename(name):
    return re.sub(r'[\\/:"*?<>|]+', "_", name)

def load_proxies():
    if not os.path.exists(PROXY_FILE):
        return []
    with open(PROXY_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]

def extract_rows(soup):
    table = soup.select_one("table#dataTable")
    if not table:
        return []

    result = []
    rows = table.select("tbody > tr")

    for row in rows:
        cols = row.find_all("td")
        if len(cols) < 6:
            continue

        link_tag = cols[0].find("a")
        result.append({
            "name": link_tag.get_text(strip=True) if link_tag else "",
            "url": f"https://zapo.ru{link_tag['href']}" if link_tag and link_tag.has_attr("href") else "",
            "year": cols[1].get_text(strip=True),
            "gearbox": cols[3].get_text(strip=True),
            "country": cols[4].get_text(strip=True),
            "description": cols[5].get_text(strip=True),
        })

    log(f"[EXTRACT_ROWS] –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(result)}")
    return result

def get_pages_total(soup):
    pages = soup.select("ul.fr-pagination li a.selectFilterPage")
    try:
        return max([int(p.text.strip()) for p in pages if p.text.strip().isdigit()], default=1)
    except:
        return 1

def setup_driver(proxy):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f'--proxy-server=socks5://{proxy}')
    log(f"[PROXY] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {proxy}")
    service = Service(CHROME_DRIVER_PATH)
    return webdriver.Chrome(service=service, options=options)

def try_requests_first(url, proxy):
    try:
        proxies = {
            "http": f"socks5h://{proxy}",
            "https": f"socks5h://{proxy}"
        }
        response = requests.get(url, headers=HEADERS, proxies=proxies, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            rows = extract_rows(soup)
            pages_total = get_pages_total(soup)
            table_found = bool(soup.select("table tr"))
            log(f"[REQUESTS] –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —á–µ—Ä–µ–∑ proxy {proxy}, rows={len(rows)}")
            return rows, proxy, table_found, pages_total
    except Exception as e:
        log(f"[REQUESTS ERROR] {proxy} ‚Äî {e}")
    return [], None, False, 0

def save_temp_file(item, rows, all_pages_loaded, pages_loaded, pages_total, table_found, modifications_expected=None):
    enriched = {k: v for k, v in item.items() if k != "proxy"}
    enriched.update({
        "modification_table": rows,
        "all_pages_loaded": all_pages_loaded,
        "pages_loaded": pages_loaded,
        "pages_total": pages_total,
        "table_found": table_found,
        "modifications_received": len(rows),
        "modifications_expected": modifications_expected
    })
    filename = os.path.join(TMP_DIR, f"{safe_filename(item['brand'])}_{safe_filename(item['model'])}.json")
    with save_lock:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(enriched, f, ensure_ascii=False, indent=2)
    log(f"[SAVE] –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")

def is_access_denied(html_text):
    return "Access denied to" in html_text or "<title>Access Denied</title>" in html_text

def prepare_requests_phase(item):
    filename = os.path.join(TMP_DIR, f"{safe_filename(item['brand'])}_{safe_filename(item['model'])}.json")
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            existing = json.load(f)
        if existing.get("all_pages_loaded", False):
            log(f"[SKIP] {item['brand']} | {item['model']} ‚Äî —É–∂–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω.")
            return

    proxy_list = load_proxies()
    used_proxies_per_item = set()
    tried_mirrors = set()

    for mirror in MIRRORS:
        url = with_mirror(item["modification_url"], mirror)
        mirror_limited = False

        for attempt in range(RETRIES_REQUESTS):
            for proxy in proxy_list:
                if proxy in used_proxies_per_item:
                    continue
                used_proxies_per_item.add(proxy)
                used_proxies.add(proxy)

                try:
                    proxies = {
                        "http": f"socks5h://{proxy}",
                        "https": f"socks5h://{proxy}"
                    }
                    
                    response = requests.get(url, headers=HEADERS, proxies=proxies, timeout=10)

                    if is_access_denied(response.text):
                        log(f"[ACCESS DENIED] {mirror} | {item['brand']} {item['model']} ‚Äî –¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â—ë–Ω, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–µ –∑–µ—Ä–∫–∞–ª–æ.")
                        continue

                    if is_rate_limited(response.text):
                        log(f"[LIMIT] {mirror} | {item['brand']} {item['model']} ‚Äî –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–µ –∑–µ—Ä–∫–∞–ª–æ.")
                        mirror_limited = True
                        break  # –≤—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–∫—Å–∏-—Ü–∏–∫–ª–∞, –Ω–æ –Ω–µ –≤—Å–µ–π —Ñ—É–Ω–∫—Ü–∏–∏

                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, "html.parser")
                        expected_modifications = extract_expected_modifications(soup)
                        rows = extract_rows(soup)
                        
                        if len(rows) == 0:
                            log(f"[EMPTY TABLE] {mirror} | {item['brand']} {item['model']} ‚Äî —Ç–∞–±–ª–∏—Ü–∞ –ø—É—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–µ –∑–µ—Ä–∫–∞–ª–æ.")
                            mirror_limited = True
                            break
                                                
                        pages_total = get_pages_total(soup)
                        table_found = bool(soup.select("table tr"))
                        log(f"[REQUESTS] OK: {mirror} —á–µ—Ä–µ–∑ {proxy}, rows={len(rows)}")

                        item["proxy"] = proxy
                        if proxy not in good_proxies:
                            good_proxies.append(proxy)

                        # –ü—Ä–∏–≤–æ–¥–∏–º URL –∫ zapo.ru –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏
                        item["modification_url"] = with_mirror(item["modification_url"], "https://zapo.ru")

                        save_temp_file(item, rows, all_pages_loaded=False,
                            pages_loaded=1, pages_total=pages_total,
                            table_found=table_found, modifications_expected=expected_modifications)
                        requests_phase_results.append(item)
                        return
                except Exception as e:
                    log(f"[REQUESTS ERROR] {mirror} | {proxy} ‚Äî {e}")

            if mirror_limited:
                break  # –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–µ—Ä–∫–∞–ª—É

        log(f"[MIRROR FAIL] {mirror} –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {item['brand']} | {item['model']}")
        tried_mirrors.add(mirror)

    failed_items.append(item)
    log(f"[FAILED REQUESTS] {item['brand']} | {item['model']} ‚Äî –≤—Å–µ –∑–µ—Ä–∫–∞–ª–∞ –∏ –ø—Ä–æ–∫—Å–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏")

def parse_with_selenium(url, proxy, start_page=2):
    all_rows = []
    visited_pages = set()
    driver = None
    try:
        driver = setup_driver(proxy)
        driver.set_page_load_timeout(PAGE_TIMEOUT)
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "dataTable")))
        soup = BeautifulSoup(driver.page_source, "html.parser")
        expected_modifications = extract_expected_modifications(soup)
        pages_total = get_pages_total_actual(soup)
        visited_pages.add("1")
        all_rows.extend(extract_rows(soup))

        while True:
            current_li = soup.select_one("ul.fr-pagination li.active span")
            if current_li:
                visited_pages.add(current_li.text.strip())

            next_a = next((a for a in soup.select("ul.fr-pagination li a.selectFilterPage")
                           if a.text.strip() not in visited_pages and int(a.text.strip()) >= start_page), None)

            if next_a:
                page_text = next_a.text.strip()
                visited_pages.add(page_text)
                try:
                    clickable = driver.find_element(By.LINK_TEXT, page_text)
                    driver.execute_script("arguments[0].click();", clickable)
                    time.sleep(2)
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    all_rows.extend(extract_rows(soup))
                except Exception as e:
                    log(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_text}: {e}")
                    break
            else:
                break

        return all_rows, True, len(visited_pages), pages_total, expected_modifications
    except Exception as e:
        log(f"[SELENIUM ERROR] –ü—Ä–æ–∫—Å–∏ {proxy} ‚Äî {e}")
        return [], False, 0, 0, None
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def selenium_phase(item):
    original_url = item["modification_url"]
    pages_loaded = item.get("pages_loaded", 1)
    pages_total = item.get("pages_total", 1)
    existing_rows = item.get("modification_table", [])
    table_found = item.get("table_found", True)

    expected_modifications = None

    proxy_list_all = [item.get("proxy")] if "proxy" in item else []
    proxy_list_all += [p for p in good_proxies + load_proxies() if p not in proxy_list_all]

    for mirror in MIRRORS:
        url = with_mirror(original_url, mirror)
        mirror_limited = False

        for proxy in proxy_list_all:
            if proxy in used_proxies:
                continue
            used_proxies.add(proxy)

            log(f"[SELENIUM] {item['brand']} | {item['model']} ‚Äî –∑–µ—Ä–∫–∞–ª–æ: {mirror}, –ø—Ä–æ–∫—Å–∏: {proxy}")
            rows, success, page_count, real_pages_total, expected_modifications = parse_with_selenium(url, proxy, start_page=pages_loaded + 1)

            if not success:
                log(f"[PROXY FAIL] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ {proxy}, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–∫—Å–∏.")
                continue  # ‚ùó –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –ø—Ä–æ–∫—Å–∏, –Ω–µ –≤—ã—Ö–æ–¥–∏–º

            if rows is not None and len(rows) == 0:
                log(f"[EMPTY TABLE] {mirror} | {item['brand']} {item['model']} ‚Äî Selenium –ø–æ–ª—É—á–∏–ª 0 —Å—Ç—Ä–æ–∫, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –¥—Ä—É–≥–æ–º—É –∑–µ—Ä–∫–∞–ª—É.")
                mirror_limited = True
                break

            if rows:
                total_rows = existing_rows + rows
                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
                seen = set()
                total_rows = [r for r in total_rows if (r['url'] not in seen and not seen.add(r['url']))]
                modifications_received = len(total_rows)
                all_loaded = (
                    (page_count + pages_loaded) >= real_pages_total or
                    (expected_modifications is not None and modifications_received >= expected_modifications)
                )
                # –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º URL
                item["modification_url"] = with_mirror(original_url, "https://zapo.ru")

                save_temp_file(item, total_rows, all_pages_loaded=all_loaded,
                               pages_loaded=page_count + pages_loaded,
                               pages_total=real_pages_total,
                               table_found=table_found,
                               modifications_expected=expected_modifications)
                log(f"[DEDUP] –£–¥–∞–ª–µ–Ω–æ {len(existing_rows) + len(rows) - len(total_rows)} –¥—É–±–ª–µ–π.")
                log(f"[FINAL] {item['brand']} | {item['model']} ‚Äî {modifications_received} —Å—Ç—Ä–æ–∫ | pages={page_count} | success={success}")
                return

            elif expected_modifications is None and page_count == 0:
                # –ü—Ä–æ–≤–µ—Ä–∏–º –ª–∏–º–∏—Ç
                try:
                    driver = setup_driver(proxy)
                    driver.get(url)
                    html = driver.page_source
                    
                    if is_access_denied(html):
                        log(f"[ACCESS DENIED] {mirror} | {item['brand']} {item['model']} ‚Äî Selenium –ø–æ–ª—É—á–∏–ª —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ç–∫–∞–∑–∞.")
                        continue
                    
                    driver.quit()
                    if is_rate_limited(html):
                        log(f"[LIMIT] {mirror} | {item['brand']} {item['model']} ‚Äî –ª–∏–º–∏—Ç –ø–æ –∑–µ—Ä–∫–∞–ª—É –≤ Selenium.")
                        mirror_limited = True
                        break
                except Exception as e:
                    log(f"[Selenium Check Error] {proxy} ‚Äî {e}")

        if mirror_limited:
            continue  # –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–µ—Ä–∫–∞–ª—É

    failed_items.append(item)
    log(f"[FAILED SELENIUM] {item['brand']} | {item['model']} ‚Äî –≤—Å–µ –∑–µ—Ä–∫–∞–ª–∞/–ø—Ä–æ–∫—Å–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏")

def main():
    all_tasks = []

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    for brand in data:
        for model in brand.get("models", []):
            all_tasks.append({
                "brand": brand.get("brand"),
                "type": brand.get("type"),
                "brand_image": brand.get("image_url"),
                "model": model.get("name"),
                "model_image": model.get("image_url"),
                "modification_url": model.get("modification_url")
            })

    # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º —Ä–∞–Ω–µ–µ –Ω–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
    if os.path.exists(FAILED_FILE):
        with open(FAILED_FILE, "r", encoding="utf-8") as f:
            failed_previous = json.load(f)
            all_tasks.extend(failed_previous)
            log(f"üîÅ –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(failed_previous)} –º–æ–¥–µ–ª–µ–π –∏–∑ {FAILED_FILE}")

    log(f"üîç –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(all_tasks)}")

    # üîπ –§–∞–∑–∞ 1 ‚Äî requests
    # with ThreadPoolExecutor(max_workers=THREADS_REQUESTS) as executor:
    #     list(tqdm(executor.map(prepare_requests_phase, all_tasks), total=len(all_tasks), desc="üåê Requests-–ø–∞—Ä—Å–∏–Ω–≥"))

    # üîπ –§–∞–∑–∞ 2 ‚Äî —á–∏—Ç–∞–µ–º TMP-—Ñ–∞–π–ª—ã, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ
    requests_phase_results.clear()
    for fname in os.listdir(TMP_DIR):
        try:
            with open(os.path.join(TMP_DIR, fname), "r", encoding="utf-8") as f:
                model_data = json.load(f)
            if not model_data.get("all_pages_loaded", False) and model_data.get("table_found", True):
                requests_phase_results.append(model_data)
        except Exception as e:
            log(f"[ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {fname}: {e}")
    log(f"üß† –ü–µ—Ä–µ–¥–∞–Ω–æ –≤ Selenium-—Ñ–∞–∑—É: {len(requests_phase_results)} –º–æ–¥–µ–ª–µ–π")

    # üîπ –§–∞–∑–∞ 3 ‚Äî Selenium
    with ThreadPoolExecutor(max_workers=THREADS_SELENIUM) as executor:
        list(tqdm(executor.map(selenium_phase, requests_phase_results), total=len(requests_phase_results), desc="üß† Selenium-–ø–∞—Ä—Å–∏–Ω–≥"))

    # üîπ –§–∞–∑–∞ 4 ‚Äî —Å–±–æ—Ä –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_data = []
    for fname in os.listdir(TMP_DIR):
        with open(os.path.join(TMP_DIR, fname), "r", encoding="utf-8") as f:
            final_data.append(json.load(f))

    total_rows = sum(len(d.get("modification_table", [])) for d in final_data)
    log(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {OUTPUT_FILE} ‚Äî –≤—Å–µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π: {total_rows}")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    if failed_items:
        with open(FAILED_FILE, "w", encoding="utf-8") as f:
            json.dump(failed_items, f, ensure_ascii=False, indent=2)
    else:
        if os.path.exists(FAILED_FILE):
            os.remove(FAILED_FILE)
        log("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã.")

if __name__ == "__main__":
    main()