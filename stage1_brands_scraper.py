import os
import requests
from bs4 import BeautifulSoup
import json
from tqdm import tqdm

LOCAL_HTML = "base.html"
REMOTE_URL = "https://zapo.ru/brandslist"
OUTPUT_JSON = "brands.json"

def fetch_html_from_site():
    try:
        print("üåê –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å —Å–∞–π—Ç–∞...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        response = requests.get(REMOTE_URL, headers=headers, timeout=20)
        response.raise_for_status()
        return response.text
    except Exception as ex:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Å–∞–π—Ç–∞: {ex}")
        return None

def clean_url(url_part):
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã, –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    return url_part.replace('\xa0', '').replace('\u200b', '').strip()

def parse_html(html):
    soup = BeautifulSoup(html, "html.parser")
    brand_links = soup.select("li.inline > a[href]")

    brands = []
    for a in tqdm(brand_links, desc="üì¶ –°–±–æ—Ä –±—Ä–µ–Ω–¥–æ–≤"):
        name = a.get_text(strip=True)
        href = clean_url(a["href"])
        full_url = f"http://zapo.ru{href}"
        brands.append({
            "name": name,
            "brand_page": full_url
        })
    return brands

def main():
    if os.path.exists(LOCAL_HTML):
        print("üìÑ –ù–∞–π–¥–µ–Ω HTML-—Ñ–∞–π–ª ‚Äî –ø–∞—Ä—Å–∏–º –ª–æ–∫–∞–ª—å–Ω–æ...")
        with open(LOCAL_HTML, "r", encoding="utf-8") as f:
            html = f.read()
    else:
        html = fetch_html_from_site()
        if not html:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML")
            return

    brands = parse_html(html)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(brands, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –±—Ä–µ–Ω–¥–æ–≤: {len(brands)} ‚Üí {OUTPUT_JSON}")

if __name__ == "__main__":
    main()
