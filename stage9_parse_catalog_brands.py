# stage1_parse_catalog_brands.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
import json
import random
from datetime import datetime
from threading import Lock
from utils import load_proxies, get_proxy_dict, proxy_lock

# === –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ===
URLS = {
    "foreign": "https://zapo.ru/auto2dV2/?action=marks&typeCatalog=CARS_FOREIGN",
    "native": "https://zapo.ru/auto2dV2/?action=marks&typeCatalog=CARS_NATIVE",
    "moto": "https://zapo.ru/auto2dV2/?action=marks&typeCatalog=MOTORCYCLE"
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

BASE_URL = "https://zapo.ru"

PROXY_FILE = "proxies_cleaned.txt"
PROXY_ALIVE_FILE = "proxies_alive.txt"
TMP_DIR = "stage9_temp_results"
LOG_DIR = "zapo_logs"
OUTPUT_FILE = "stage9_brands.json"
RETRIES = 10

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(TMP_DIR, exist_ok=True)
log_file_path = os.path.join(LOG_DIR, f"brands_parse_log_{datetime.now():%Y%m%d_%H%M%S}.txt")
log_lock = Lock()
save_lock = Lock()

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
def log(message: str):
    print(message)
    with log_lock:
        with open(log_file_path, "a", encoding="utf-8") as f:
            f.write(message + "\n")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–∫—Å–∏ ===
proxies = load_proxies(PROXY_FILE, PROXY_ALIVE_FILE)
working_proxies = []

# === –ü–æ–ª—É—á–µ–Ω–∏–µ HTML —á–µ—Ä–µ–∑ SOCKS5 –ø—Ä–æ–∫—Å–∏ ===
def fetch_html(url):
    """Download a URL using available proxies with retries."""
    global proxies, working_proxies
    for attempt in range(1, RETRIES + 1):
        with proxy_lock:
            proxy_list = proxies.copy()
        random.shuffle(proxy_list)

        while proxy_list:
            proxy = proxy_list.pop()
            try:
                response = requests.get(
                    url, headers=HEADERS,
                    proxies=get_proxy_dict(proxy),
                    timeout=10
                )
                response.raise_for_status()
                with proxy_lock:
                    if proxy not in working_proxies:
                        working_proxies.append(proxy)
                return response.text
            except Exception as e:
                log(f"[PROXY ERROR] {proxy} ‚Äî {e}")
                with proxy_lock:
                    if proxy in proxies:
                        proxies.remove(proxy)

        try:
            log(f"[ATTEMPT {attempt}] –ü—Ä–æ–±—É–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏...")
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            log(f"[ERROR] –ü–æ–ø—ã—Ç–∫–∞ {attempt} –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")

    log(f"[FAILED] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å: {url} –ø–æ—Å–ª–µ {RETRIES} –ø–æ–ø—ã—Ç–æ–∫")
    return None

# === –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (foreign/native/moto) ===
def parse_catalog(url):
    for attempt in range(1, RETRIES + 1):
        html = fetch_html(url)
        if not html:
            log(f"[RETRY {attempt}] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å HTML: {url}")
            continue

        soup = BeautifulSoup(html, "html.parser")
        blocks = soup.select("a.catalogAuto2dMarkLink")

        if blocks:
            results = []
            for a_tag in blocks:
                name_tag = a_tag.select_one("span.catalogAuto2dMarkName")
                img_tag = a_tag.select_one("img")

                results.append({
                    "name": name_tag.get_text(strip=True) if name_tag else "",
                    "image_url": img_tag["src"] if img_tag else "",
                    "link": urljoin(BASE_URL, a_tag["href"])
                })

            return results

        else:
            log(f"[RETRY {attempt}] –ë–ª–æ–∫–∏ –±—Ä–µ–Ω–¥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ HTML: {url}")

    log(f"[FAILED] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ —Å—Å—ã–ª–∫–µ: {url}")
    return []

# === –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ===
def main():
    final_result = {}

    for key, url in URLS.items():
        log(f"üîç –ü–∞—Ä—Å–∏–º: {key}")
        data = parse_catalog(url)
        final_result[key] = data
        log(f"[OK] {key} ‚Äî {len(data)} –±—Ä–µ–Ω–¥–æ–≤")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π
        tmp_path = os.path.join(TMP_DIR, f"{key}.json")
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    # –§–∏–Ω–∞–ª—å–Ω—ã–π JSON
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)

    log(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_FILE}")
    log(f"üìù –†–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏: {len(working_proxies)}")

    with open(PROXY_ALIVE_FILE, "w", encoding="utf-8") as f:
        for proxy in working_proxies:
            f.write(proxy + "\n")

if __name__ == "__main__":
    main()
