import os
import re
import json
import gzip
import xml.etree.ElementTree as ET
from urllib.parse import quote
from itertools import product
from collections import defaultdict
from datetime import datetime
from typing import List, Dict
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils import load_proxies, fetch_with_proxies

# ---------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ----------
GROUPS_FILE = "groups.json"
TEMP_DIR = "stage13_temp_results"
OUTPUT_DIR = "stage13_temp_results/sitemaps_output"
PROXY_FILE = "proxies_cleaned.txt"
PROXY_ALIVE_FILE = "proxies_alive.txt"
BASE_URL = "https://zapo.ru"
MAX_URLS = 50000
MAX_XML_SIZE = 8 * 1024 * 1024
REQUEST_TIMEOUT = 15
RETRIES = 10
HEADERS = {"User-Agent": "Mozilla/5.0"}

FILTER_LIMIT = 5
THREADS = 25

os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ---------- –ü—Ä–æ–∫—Å–∏ ----------
proxies = load_proxies(PROXY_FILE, PROXY_ALIVE_FILE, logger=print)
working_proxies: List[str] = []

def reload_proxies():
    return load_proxies(PROXY_FILE, PROXY_ALIVE_FILE, check_alive=True, logger=print)

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ HTML ----------
def download_and_save_html(group_id: str) -> str:
    url = f"{BASE_URL}/{group_id}_catalog"
    html_path = os.path.join(TEMP_DIR, f"{group_id}.html")
    if os.path.exists(html_path):
        with open(html_path, "r", encoding="utf-8") as f:
            html = f.read()
            if "<form" in html:
                return html
            os.remove(html_path)

    html, _ = fetch_with_proxies(
        url, proxies, working_proxies,
        headers=HEADERS, retries=RETRIES,
        timeout=REQUEST_TIMEOUT,
        logger=print,
        reload_proxies=reload_proxies,
    )

    if not html or "<form" not in html:
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å HTML –¥–ª—è –≥—Ä—É–ø–ø—ã: {group_id}")

    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html)
    return html

# ---------- –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∏–ª—å—Ç—Ä–æ–≤ ----------
def parse_filters(html: str) -> Dict[str, List[str]]:
    soup = BeautifulSoup(html, "lxml")
    form = soup.find("form", id="catalog-form")
    if not form:
        raise ValueError("–§–æ—Ä–º–∞ —Å id='catalog-form' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    checkboxes = form.find_all("input", {"type": "checkbox", "name": True})
    filters = defaultdict(list)
    for cb in checkboxes:
        name, value = cb.get("name"), cb.get("value")
        m = re.search(r"property\[(.+?)\]", name or "")
        if m and value:
            filters[m.group(1)].append(value)
    return filters

# ---------- –í—ã–±–æ—Ä –∫–ª—é—á–µ–π ----------
def select_filter_keys(all_keys: List[str], limit: int = FILTER_LIMIT) -> List[str]:
    return all_keys[:limit] if limit and len(all_keys) > limit else all_keys

# ---------- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ ----------
def generate_links(filters: Dict[str, List[str]], keys: List[str], group_id: str) -> List[str]:
    values = [filters.get(k, []) for k in keys]
    links = []
    for combo in product(*values):
        url = f"{BASE_URL}/{group_id}_catalog?goods_group={group_id}&action=search&viewMode=tile&resultMode=5&hidePriceIn=1"
        for k, v in zip(keys, combo):
            url += f"&property[{k}][]={quote(v)}"
        links.append(url)
    return links

# ---------- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ sitemap-—Ñ–∞–π–ª–æ–≤ ----------
def save_sitemaps(urls: List[str], group_id: str) -> List[str]:
    now = datetime.now().isoformat(timespec="seconds") + "+03:00"
    files, chunk, size, index = [], [], 0, 1

    def write_chunk(part_urls: List[str], part_index: int) -> str:
        urlset = ET.Element("urlset", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
        for u in part_urls:
            url_el = ET.SubElement(urlset, "url")
            ET.SubElement(url_el, "loc").text = u
            ET.SubElement(url_el, "lastmod").text = now
            ET.SubElement(url_el, "changefreq").text = "weekly"
        path = os.path.join(OUTPUT_DIR, f"sitemap_{group_id}_{part_index}.xml")
        ET.ElementTree(urlset).write(path, encoding="utf-8", xml_declaration=True)
        gz_path = path + ".gz"
        with open(path, "rb") as f_in, gzip.open(gz_path, "wb") as f_out:
            f_out.writelines(f_in)
        os.remove(path)
        return gz_path

    for url in urls:
        size += len(url.encode()) + 100
        chunk.append(url)
        if len(chunk) >= MAX_URLS or size >= MAX_XML_SIZE:
            files.append(write_chunk(chunk, index))
            chunk, size = [], 0
            index += 1

    if chunk:
        files.append(write_chunk(chunk, index))

    return files

# ---------- –ò–Ω–¥–µ–∫—Å–Ω—ã–π sitemap ----------
def generate_index(gz_files: List[str]):
    now = datetime.now().isoformat(timespec="seconds") + "+03:00"
    root = ET.Element("sitemapindex", xmlns="http://www.sitemaps.org/schemas/sitemap/0.9")
    for f in gz_files:
        sm = ET.SubElement(root, "sitemap")
        ET.SubElement(sm, "loc").text = f"{BASE_URL}/{os.path.basename(f)}"
        ET.SubElement(sm, "lastmod").text = now
    ET.ElementTree(root).write(
        os.path.join(OUTPUT_DIR, "sitemap_catalog_index.xml"),
        encoding="utf-8",
        xml_declaration=True
    )

# ---------- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã ----------
def process_group(group: Dict) -> List[str]:
    group_id = group["id"]

    # ‚úÖ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    expected_file = os.path.join(OUTPUT_DIR, f"sitemap_{group_id}_1.xml.gz")
    if os.path.exists(expected_file):
        print(f"‚è≠Ô∏è {group_id} —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω, –ø—Ä–æ–ø—É—Å–∫...")
        return []

    try:
        print(f"üîç {group_id}")
        html = download_and_save_html(group_id)
        filters = parse_filters(html)
        all_keys = list(filters.keys())
        selected = select_filter_keys(all_keys)
        urls = generate_links(filters, selected, group_id)
        print(f"‚úÖ {group_id}: {len(urls)} —Å—Å—ã–ª–æ–∫ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º {selected}")
        return save_sitemaps(urls, group_id)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –≥—Ä—É–ø–ø–µ {group_id}: {e}")
        return []

# ---------- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ----------
def main():
    with open(GROUPS_FILE, "r", encoding="utf-8") as f:
        groups = json.load(f)

    all_gz = []
    with ThreadPoolExecutor(max_workers=THREADS) as executor:
        futures = [executor.submit(process_group, g) for g in groups]
        for future in as_completed(futures):
            all_gz.extend(future.result())

    generate_index(all_gz)
    print("üèÅ Sitemap –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

if __name__ == "__main__":
    main()
